{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train literal listener and speaker to use DNC representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../color-evaluation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dnc\n",
    "from monroe_data import MonroeData, MonroeDataEntry, Color # last two for reading pkl file\n",
    "import caption_featurizers\n",
    "from color_featurizers import ColorFeaturizer, color_phi_fourier\n",
    "from models import LiteralListener, LiteralSpeaker, CaptionEncoder, CaptionGenerator, PytorchModel, ColorEncoder, BeamNode\n",
    "from experiment import FeatureHandler\n",
    "import scipy.stats as stats\n",
    "from evaluation import score_model, Score\n",
    "\n",
    "import numpy as np\n",
    "from queue import PriorityQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in DNC\n",
    "class DNCEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, color_dim):\n",
    "        super(DNCEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = dnc.DNC(\n",
    "          input_size=embed_dim + color_dim,\n",
    "          hidden_size=128,\n",
    "          rnn_type='lstm',\n",
    "          num_layers=4,\n",
    "          nr_cells=100,\n",
    "          cell_size=32,\n",
    "          read_heads=4,\n",
    "          batch_first=True,\n",
    "          gpu_id=-1,\n",
    "          debug=True\n",
    "        )\n",
    "        self.decoder = nn.Linear(embed_dim + color_dim, vocab_size) # don't predict over padding tag\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, caption, color):\n",
    "        embeds = self.embed(caption)\n",
    "        color_reps = color.repeat(1, caption.shape[1], 1) # repeat color for number of tokens in captions\n",
    "        # concatenate colors to caption\n",
    "        inputs = torch.cat((embeds, color_reps), dim=2) # cat along the innermost dimension\n",
    "        # dnc magic\n",
    "        (controller_hidden, memory, read_vectors) = (None, None, None)\n",
    "        output, (controller_hidden, memory, read_vectors), debug_memory = \\\n",
    "          self.rnn(inputs, (controller_hidden, memory, read_vectors), reset_experience=True)\n",
    "\n",
    "        result = self.decoder(output)\n",
    "        return self.logsoftmax(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNCSpeaker(PytorchModel):\n",
    "    \n",
    "    def predict(self, X, sample=1, beam_width=5):\n",
    "        all_tokens = []\n",
    "        self.model.eval()\n",
    "        max_gen_len = 20\n",
    "    \n",
    "        self.model.eval()\n",
    "        if not torch.cuda.is_available():\n",
    "            torch.manual_seed(10) # for determinism\n",
    "        else:\n",
    "            torch.cuda.manual_seed_all(10)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for i, feature in enumerate(X):\n",
    "                caption, colors = feature\n",
    "                caption = torch.tensor(caption, dtype=torch.long)\n",
    "                colors = torch.tensor(colors, dtype=torch.float)\n",
    "\n",
    "    \n",
    "                beam_nodes = PriorityQueue()\n",
    "                ended_list = []\n",
    "    \n",
    "                tokens = caption[:, 0].view(-1, 1) # begin at start token\n",
    "                #print(tokens)\n",
    "                start = BeamNode(0, tokens, False)\n",
    "                beam_nodes.put(start)\n",
    "    \n",
    "                for i in range(max_gen_len + 1):\n",
    "                    node = beam_nodes.get()\n",
    "                    if node.ended:\n",
    "                        ended_list.append(np.array(node.tokens[0].numpy()))\n",
    "                        if len(ended_list) == sample:\n",
    "                            break\n",
    "                    else:\n",
    "                        tokens = node.tokens\n",
    "                        vocab_preds = self.model(tokens, colors)[:,-1:,:] # just distribution over last token\n",
    "                        log_probs, prediction_indices = vocab_preds.topk(beam_width, dim=2)  # taking the topk predictions\n",
    "                        for j in range(beam_width):\n",
    "                            prediction_index = prediction_indices[:,-1,j:j+1] # a single prediction\n",
    "                            log_prob = log_probs[0][0][j].item()\n",
    "                            updated_tokens = tokens.clone()\n",
    "                            updated_tokens = torch.cat((updated_tokens, prediction_index), dim=1)\n",
    "                            updated_log_prob = node.log_prob + log_prob\n",
    "                            ended = ((i == max_gen_len - 1) or (prediction_index.item() == caption[:, -1].item()))#.view(-1, 1)))\n",
    "                            new_node = BeamNode(updated_log_prob, updated_tokens, ended)\n",
    "                            beam_nodes.put(new_node)\n",
    "                if sample == 1: # for backwards compatability\n",
    "                    all_tokens.append(np.array(ended_list))\n",
    "                else:\n",
    "                    all_tokens.append(ended_list)\n",
    "        return all_tokens\n",
    "    \n",
    "    def train_iter(self, caption_tensor, color_tensor, target_tensor, criterion):\n",
    "        model_output = self.model(caption_tensor, color_tensor)\n",
    "        \n",
    "        #model_output = model_output[:, :-1, :].squeeze(0)\n",
    "        model_output = model_output.view(-1, self.model.vocab_size)\n",
    "        target_tensor = target_tensor.view(-1)\n",
    "        loss = criterion(model_output, target_tensor)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_phi_fourier(color_list, space=\"hsv\", resolution=3):\n",
    "    \"\"\"\n",
    "    This is lifted straight but modified to take straight hsv from https://github.com/futurulus/colors-in-context/blob/2e7b830668cd039830154e7e8f211c6d4415d30f/vectorizers.py#L650\n",
    "    Haven't figured out how it works yet. but it seems to be the only feature function to get somewhat decent results so far\n",
    "    \"\"\"\n",
    "#     if space not in (\"rgb_norm\", \"hsv_norm\"):\n",
    "#         print(\"Space must be rgb_norm or hsv_norm to use fourier transform\")\n",
    "#         return None\n",
    "\n",
    "    resolution = [resolution for _ in color_list]\n",
    "    colors = np.array([color_list])\n",
    "    # I'm pretty sure ranges aren't actually used anywhere...\n",
    "    if space == \"rgb_norm\":\n",
    "        ranges = np.array([256, 256, 256])\n",
    "    else:\n",
    "        ranges = np.array([361, 101, 101])\n",
    "\n",
    "    color_list = [color_list[0]/360, color_list[1]/100, color_list[2]/100]\n",
    "    \n",
    "    # Using a Fourier representation causes colors at the boundary of the\n",
    "    # space to behave as if the space is toroidal: red = 255 would be\n",
    "    # about the same as red = 0. We don't want this... so we divide\n",
    "    # all of the rgb values by 2. (If we were doing this with hsv\n",
    "    # we wouldn't divide the h value by two becaus it actually is\n",
    "    # polar, so 360 is close to 0 (both are red)\n",
    "    if space == \"rgb_norm\":\n",
    "        xyz = colors / 2\n",
    "    else:\n",
    "        xyz = colors / 2\n",
    "        xyz[:, 0] *= 2 # this is the 'h' of the 'hsv'\n",
    "\n",
    "    ax, ay, az = [np.arange(0, g) for g, r in zip(resolution, ranges)]\n",
    "    gx, gy, gz = np.meshgrid(ax, ay, az)\n",
    "\n",
    "    arg = (np.multiply.outer(xyz[:, 0], gx) +\n",
    "           np.multiply.outer(xyz[:, 1], gy) +\n",
    "           np.multiply.outer(xyz[:, 2], gz))\n",
    "\n",
    "    repr_complex = np.exp(-2j * np.pi * (arg % 1.0)).swapaxes(1, 2).reshape((xyz.shape[0], -1))\n",
    "    result = np.hstack([repr_complex.real, repr_complex.imag]).astype(np.float32)\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get DNC\n",
    "dnc_speaker = DNCSpeaker(DNCEncoder, num_epochs=5)\n",
    "dnc_speaker.init_model(vocab_size = 368, embed_dim=100, color_dim=54) # vocab size just copied from dnc training notebook\n",
    "# load model manually because we need to set \"map_location\" to cpu\n",
    "dnc_speaker.model.load_state_dict(torch.load(\"checkpoint_4.params\", map_location='cpu'))\n",
    "#dnc_speaker.load_model(\"./model_checkpoint_4.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "monroe_train_data = MonroeData(\"../color-evaluation/data/csv/train_corpus_monroe.csv\", \"../color-evaluation/data/entries/train_entries_monroe.pkl\")\n",
    "monroe_dev_data = MonroeData(\"../color-evaluation/data/csv/dev_corpus_monroe.csv\", \"../color-evaluation/data/entries/dev_entries_monroe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dnc color featurizer:\n",
    "def dnc_phi(color_list, space):\n",
    "    if space != \"hsv\":\n",
    "        print(\"Space must be hsv to use dnc\")\n",
    "        return None\n",
    "    # we're going to do a greedy search for the tokens and then max-pool their embeddings\n",
    "    features = [\n",
    "        [np.array([[0, 1]]), np.array([color_phi_fourier(color_list)])]\n",
    "    ]\n",
    "    #print(features)\n",
    "    predicted_tokens = dnc_speaker.predict(features) # looks like [array([[  0, 123,   1]])]\n",
    "    \n",
    "    predicted_tokens = predicted_tokens[0].flatten() # looks like array([  0, 123,   1])\n",
    "    \n",
    "    predicted_tokens = predicted_tokens[1:-1] # get rid of nasty start and end tokens\n",
    "    \n",
    "    # get embeddings for predicted tokens\n",
    "    #print(\"hello\", predicted_tokens)\n",
    "    embeds = dnc_speaker.model.embed(torch.from_numpy(np.array(predicted_tokens)))\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([[0, 1]]), array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.]], dtype=float32)]]\n",
      "tensor([[0]])\n",
      "hello [120]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0100, -0.9641, -1.5174,  0.0861,  1.5254,  1.0565,  0.4181,  0.1787,\n",
       "          0.6285, -1.9980, -0.4956,  0.3684,  0.7323,  0.9005,  0.4452,  0.9927,\n",
       "         -0.8477,  1.3443, -0.5014, -1.2765, -0.6567,  0.4856,  0.4202, -0.3767,\n",
       "         -1.8097,  0.5443, -0.1397,  0.4094,  0.1381,  0.8562, -0.1073, -0.2321,\n",
       "          0.1718,  0.0061, -0.3351, -0.0614,  1.2263,  1.3344,  0.3503, -0.4874,\n",
       "         -0.7214, -0.4114,  1.2891, -1.2204,  1.3222, -0.1335, -0.2869,  1.0041,\n",
       "         -1.9206,  1.0088,  0.4697,  0.6737,  1.4490, -1.4709,  0.3612, -0.3839,\n",
       "         -1.1166,  0.5365, -0.4841,  0.5475, -1.6019,  1.1423, -0.9571,  2.1764,\n",
       "         -0.6859,  1.9283,  0.6891,  1.1430, -0.4513, -0.3581, -1.1672, -1.0731,\n",
       "          0.1990,  0.0097,  1.9685,  0.5761, -0.2622,  0.4326,  0.5301,  0.3559,\n",
       "         -1.5529, -0.5831,  0.8287,  0.9119,  1.2720, -0.5678, -0.7216,  0.2540,\n",
       "          0.6690, -1.2522, -1.0299, -0.3800,  0.4937,  0.6474, -0.5182, -2.2360,\n",
       "          0.8794, -0.2466, -0.1294,  1.5744]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnc_phi([100.0, 30.0, 30.0], space=\"hsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = [\n",
    "        [np.array([[0, 1]]), np.array([[100.0, 30.0, 30.0]])]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[0, 1]]), array([[100.,  30.,  30.]])]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phi = color_phi_fourier(np.array([100.,  30.,  30. ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[  0, 120,   1]])]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnc_speaker.predict([[np.array([[0, 2, 1]]), test_phi]], beam_width=5, sample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  0, 367, 367, 367, 367,   2,   2,   2,   2, 367, 367, 367, 367,\n",
       "         367, 367, 367, 365, 365,   2,   2,   2]])]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[0, 1]])[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([], dtype=float64)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[0, 1]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 2. Got 2 and 1 in dimension 0 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/wheel_3.6/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1333",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-264eb05e5895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnc_phi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-a9cde9b4202a>\u001b[0m in \u001b[0;36mdnc_phi\u001b[0;34m(color_list, space)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolor_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     ]\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpredicted_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnc_speaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# looks like [array([[  0, 123,   1]])]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpredicted_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# looks like array([  0, 123,   1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-9ba6efc1acf6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, sample, beam_width)\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                         \u001b[0mvocab_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# just distribution over last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# taking the topk predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benjaminnewman/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-44cd44b861f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, caption, color)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcolor_reps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# repeat color for number of tokens in captions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# concatenate colors to caption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cat along the innermost dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# dnc magic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mcontroller_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_vectors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 2. Got 2 and 1 in dimension 0 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/wheel_3.6/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1333"
     ]
    }
   ],
   "source": [
    "embeds = dnc_phi([100.0, 30.0, 30.0], space=\"hsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing featurizers\n"
     ]
    }
   ],
   "source": [
    "# OK because DNC isn't working, let's do this with the regular literal speaker/listener\n",
    "\n",
    "print(\"Initializing featurizers\")\n",
    "caption_phi = caption_featurizers.CaptionFeaturizer(tokenizer=caption_featurizers.EndingTokenizer) # Use with parameter files that end in `endings_tkn` - using endings tokenizer to separate endings like \"ish\" and \"er\"\n",
    "color_phi = ColorFeaturizer(dnc_phi, \"hsv\", normalized=False)\n",
    "\n",
    "\n",
    "feature_handler = FeatureHandler(monroe_train_data, monroe_dev_data, caption_phi, color_phi,\n",
    "                                randomized_colors=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining training features\n",
      "CPU times: user 1h 4min 43s, sys: 2min 22s, total: 1h 7min 5s\n",
      "Wall time: 1h 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Obtaining training features\") # get features even if you're runnning the pretrained model for example\n",
    "train_features = feature_handler.train_features()\n",
    "train_targets = feature_handler.train_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"dnc_lit_listener_and_speaker_train_features.params\", \"wb\") as file:\n",
    "    pickle.dump(train_features, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(*[i.detach().numpy() for i in train_features[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose([i.detach().numpy() for i in train_features[0][1]][0][0], [i.detach().numpy() for i in train_features[8784][1]][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2, 3, 4, 5, 6]),\n",
       " array([tensor([[-0.0100, -0.9641, -1.5174,  0.0861,  1.5254,  1.0565,  0.4181,  0.1787,\n",
       "           0.6285, -1.9980, -0.4956,  0.3684,  0.7323,  0.9005,  0.4452,  0.9927,\n",
       "          -0.8477,  1.3443, -0.5014, -1.2765, -0.6567,  0.4856,  0.4202, -0.3767,\n",
       "          -1.8097,  0.5443, -0.1397,  0.4094,  0.1381,  0.8562, -0.1073, -0.2321,\n",
       "           0.1718,  0.0061, -0.3351, -0.0614,  1.2263,  1.3344,  0.3503, -0.4874,\n",
       "          -0.7214, -0.4114,  1.2891, -1.2204,  1.3222, -0.1335, -0.2869,  1.0041,\n",
       "          -1.9206,  1.0088,  0.4697,  0.6737,  1.4490, -1.4709,  0.3612, -0.3839,\n",
       "          -1.1166,  0.5365, -0.4841,  0.5475, -1.6019,  1.1423, -0.9571,  2.1764,\n",
       "          -0.6859,  1.9283,  0.6891,  1.1430, -0.4513, -0.3581, -1.1672, -1.0731,\n",
       "           0.1990,  0.0097,  1.9685,  0.5761, -0.2622,  0.4326,  0.5301,  0.3559,\n",
       "          -1.5529, -0.5831,  0.8287,  0.9119,  1.2720, -0.5678, -0.7216,  0.2540,\n",
       "           0.6690, -1.2522, -1.0299, -0.3800,  0.4937,  0.6474, -0.5182, -2.2360,\n",
       "           0.8794, -0.2466, -0.1294,  1.5744]], grad_fn=<EmbeddingBackward>),\n",
       "        tensor([[-0.0100, -0.9641, -1.5174,  0.0861,  1.5254,  1.0565,  0.4181,  0.1787,\n",
       "           0.6285, -1.9980, -0.4956,  0.3684,  0.7323,  0.9005,  0.4452,  0.9927,\n",
       "          -0.8477,  1.3443, -0.5014, -1.2765, -0.6567,  0.4856,  0.4202, -0.3767,\n",
       "          -1.8097,  0.5443, -0.1397,  0.4094,  0.1381,  0.8562, -0.1073, -0.2321,\n",
       "           0.1718,  0.0061, -0.3351, -0.0614,  1.2263,  1.3344,  0.3503, -0.4874,\n",
       "          -0.7214, -0.4114,  1.2891, -1.2204,  1.3222, -0.1335, -0.2869,  1.0041,\n",
       "          -1.9206,  1.0088,  0.4697,  0.6737,  1.4490, -1.4709,  0.3612, -0.3839,\n",
       "          -1.1166,  0.5365, -0.4841,  0.5475, -1.6019,  1.1423, -0.9571,  2.1764,\n",
       "          -0.6859,  1.9283,  0.6891,  1.1430, -0.4513, -0.3581, -1.1672, -1.0731,\n",
       "           0.1990,  0.0097,  1.9685,  0.5761, -0.2622,  0.4326,  0.5301,  0.3559,\n",
       "          -1.5529, -0.5831,  0.8287,  0.9119,  1.2720, -0.5678, -0.7216,  0.2540,\n",
       "           0.6690, -1.2522, -1.0299, -0.3800,  0.4937,  0.6474, -0.5182, -2.2360,\n",
       "           0.8794, -0.2466, -0.1294,  1.5744]], grad_fn=<EmbeddingBackward>),\n",
       "        tensor([[-0.0100, -0.9641, -1.5174,  0.0861,  1.5254,  1.0565,  0.4181,  0.1787,\n",
       "           0.6285, -1.9980, -0.4956,  0.3684,  0.7323,  0.9005,  0.4452,  0.9927,\n",
       "          -0.8477,  1.3443, -0.5014, -1.2765, -0.6567,  0.4856,  0.4202, -0.3767,\n",
       "          -1.8097,  0.5443, -0.1397,  0.4094,  0.1381,  0.8562, -0.1073, -0.2321,\n",
       "           0.1718,  0.0061, -0.3351, -0.0614,  1.2263,  1.3344,  0.3503, -0.4874,\n",
       "          -0.7214, -0.4114,  1.2891, -1.2204,  1.3222, -0.1335, -0.2869,  1.0041,\n",
       "          -1.9206,  1.0088,  0.4697,  0.6737,  1.4490, -1.4709,  0.3612, -0.3839,\n",
       "          -1.1166,  0.5365, -0.4841,  0.5475, -1.6019,  1.1423, -0.9571,  2.1764,\n",
       "          -0.6859,  1.9283,  0.6891,  1.1430, -0.4513, -0.3581, -1.1672, -1.0731,\n",
       "           0.1990,  0.0097,  1.9685,  0.5761, -0.2622,  0.4326,  0.5301,  0.3559,\n",
       "          -1.5529, -0.5831,  0.8287,  0.9119,  1.2720, -0.5678, -0.7216,  0.2540,\n",
       "           0.6690, -1.2522, -1.0299, -0.3800,  0.4937,  0.6474, -0.5182, -2.2360,\n",
       "           0.8794, -0.2466, -0.1294,  1.5744]], grad_fn=<EmbeddingBackward>)],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_features = feature_handler.test_features()\n",
    "assess_targets = feature_handler.test_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0, 96,  6]),\n",
       " array([[ 1.0000000e+00,  2.0711137e-01, -9.1420978e-01, -1.2241068e-01,\n",
       "         -9.9631262e-01, -2.9028466e-01, -9.7003126e-01,  3.6807224e-02,\n",
       "          9.8527765e-01, -2.0711137e-01, -1.0000000e+00, -2.0711137e-01,\n",
       "         -9.4560730e-01,  1.2241068e-01,  9.9631262e-01,  4.3861625e-01,\n",
       "          9.7003126e-01, -3.6807224e-02, -9.1420978e-01,  2.0711137e-01,\n",
       "          1.0000000e+00,  5.1410276e-01,  9.4560730e-01, -1.2241068e-01,\n",
       "          7.8834641e-01, -4.3861625e-01, -9.7003126e-01,  0.0000000e+00,\n",
       "         -9.7831738e-01, -4.0524131e-01, -9.9247956e-01, -8.5797310e-02,\n",
       "          9.5694035e-01,  2.4298018e-01,  9.9932235e-01,  1.7096189e-01,\n",
       "         -9.7831738e-01, -1.2246469e-16,  9.7831738e-01,  3.2531029e-01,\n",
       "          9.9247956e-01,  8.5797310e-02,  8.9867449e-01, -2.4298018e-01,\n",
       "         -9.9932235e-01,  4.0524131e-01,  9.7831738e-01,  0.0000000e+00,\n",
       "          8.5772860e-01, -3.2531029e-01, -9.9247956e-01, -6.1523157e-01,\n",
       "         -8.9867449e-01,  2.4298018e-01],\n",
       "        [ 1.0000000e+00,  5.1410276e-01, -4.7139674e-01,  7.3564567e-02,\n",
       "         -8.1758481e-01, -9.1420978e-01, -9.8917651e-01, -6.3439327e-01,\n",
       "          3.3688986e-01, -5.1410276e-01, -1.0000000e+00, -5.1410276e-01,\n",
       "         -8.9322430e-01, -7.3564567e-02,  8.1758481e-01,  3.8268343e-01,\n",
       "          9.8917651e-01,  6.3439327e-01, -4.7139674e-01,  5.1410276e-01,\n",
       "          1.0000000e+00,  8.4485358e-01,  8.9322430e-01,  7.3564567e-02,\n",
       "          5.9569931e-01, -3.8268343e-01, -9.8917651e-01,  0.0000000e+00,\n",
       "         -8.5772860e-01, -8.8192129e-01, -9.9729043e-01, -5.7580817e-01,\n",
       "          4.0524131e-01, -1.4673047e-01,  7.7301043e-01,  9.4154406e-01,\n",
       "         -8.5772860e-01, -1.2246469e-16,  8.5772860e-01,  4.4961134e-01,\n",
       "          9.9729043e-01,  5.7580817e-01,  9.2387950e-01,  1.4673047e-01,\n",
       "         -7.7301043e-01,  8.8192129e-01,  8.5772860e-01,  0.0000000e+00,\n",
       "          5.3499764e-01, -4.4961134e-01, -9.9729043e-01, -8.0320752e-01,\n",
       "         -9.2387950e-01, -1.4673047e-01],\n",
       "        [ 1.0000000e+00, -7.8834641e-01,  2.4298018e-01, -8.7008697e-01,\n",
       "          3.8268343e-01,  2.6671275e-01,  5.1410276e-01,  1.2241068e-01,\n",
       "         -7.0710677e-01,  8.7008697e-01, -9.8917651e-01,  6.8954057e-01,\n",
       "         -1.0000000e+00,  7.8834641e-01, -2.4298018e-01,  8.7008697e-01,\n",
       "         -3.8268343e-01, -2.6671275e-01,  5.1410276e-01, -9.3299282e-01,\n",
       "          9.5694035e-01, -8.7008697e-01,  9.8917651e-01, -6.8954057e-01,\n",
       "          1.0000000e+00, -7.8834641e-01,  2.4298018e-01,  0.0000000e+00,\n",
       "         -6.1523157e-01,  9.7003126e-01, -4.9289820e-01,  9.2387950e-01,\n",
       "         -9.6377605e-01,  8.5772860e-01, -9.9247956e-01,  7.0710677e-01,\n",
       "         -4.9289820e-01, -1.4673047e-01,  7.2424710e-01, -1.2246469e-16,\n",
       "          6.1523157e-01, -9.7003126e-01,  4.9289820e-01, -9.2387950e-01,\n",
       "          9.6377605e-01, -8.5772860e-01,  3.5989505e-01,  2.9028466e-01,\n",
       "          4.9289820e-01,  1.4673047e-01, -7.2424710e-01,  0.0000000e+00,\n",
       "         -6.1523157e-01,  9.7003126e-01]], dtype=float32)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assess_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in assess_features:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
