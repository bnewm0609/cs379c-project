{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train literal listener and speaker to use DNC representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../color-evaluation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dnc\n",
    "from monroe_data import MonroeData, MonroeDataEntry, Color # last two for reading pkl file\n",
    "import caption_featurizers\n",
    "from color_featurizers import ColorFeaturizer, color_phi_fourier\n",
    "from models import LiteralListener, LiteralSpeaker, CaptionEncoder, CaptionGenerator, PytorchModel, ColorEncoder, BeamNode\n",
    "from experiment import FeatureHandler\n",
    "import scipy.stats as stats\n",
    "from evaluation import score_model, Score\n",
    "\n",
    "import numpy as np\n",
    "from queue import PriorityQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in DNC\n",
    "class DNCEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, color_dim):\n",
    "        super(DNCEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = dnc.DNC(\n",
    "          input_size=embed_dim + color_dim,\n",
    "          hidden_size=128,\n",
    "          rnn_type='lstm',\n",
    "          num_layers=4,\n",
    "          nr_cells=100,\n",
    "          cell_size=32,\n",
    "          read_heads=4,\n",
    "          batch_first=True,\n",
    "          gpu_id=-1,\n",
    "          debug=True\n",
    "        )\n",
    "        self.decoder = nn.Linear(embed_dim + color_dim, vocab_size) # don't predict over padding tag\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, caption, color):\n",
    "        embeds = self.embed(caption)\n",
    "        color_reps = color.repeat(1, caption.shape[1], 1) # repeat color for number of tokens in captions\n",
    "        # concatenate colors to caption\n",
    "        inputs = torch.cat((embeds, color_reps), dim=2) # cat along the innermost dimension\n",
    "        # dnc magic\n",
    "        (controller_hidden, memory, read_vectors) = (None, None, None)\n",
    "        output, (controller_hidden, memory, read_vectors), debug_memory = \\\n",
    "          self.rnn(inputs, (controller_hidden, memory, read_vectors), reset_experience=True)\n",
    "\n",
    "        result = self.decoder(output)\n",
    "        return self.logsoftmax(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNCSpeaker(PytorchModel):\n",
    "    \n",
    "    def predict(self, X, sample=1, beam_width=5):\n",
    "        all_tokens = []\n",
    "        self.model.eval()\n",
    "        max_gen_len = 20\n",
    "    \n",
    "        self.model.eval()\n",
    "        if not torch.cuda.is_available():\n",
    "            torch.manual_seed(10) # for determinism\n",
    "        else:\n",
    "            torch.cuda.manual_seed_all(10)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for i, feature in enumerate(X):\n",
    "                caption, colors = feature\n",
    "                caption = torch.tensor(caption, dtype=torch.long)\n",
    "                colors = torch.tensor(colors, dtype=torch.float)\n",
    "\n",
    "    \n",
    "                beam_nodes = PriorityQueue()\n",
    "                ended_list = []\n",
    "    \n",
    "                tokens = caption[:, 0].view(-1, 1) # begin at start token\n",
    "                print(tokens)\n",
    "                start = BeamNode(0, tokens, False)\n",
    "                beam_nodes.put(start)\n",
    "    \n",
    "                for i in range(max_gen_len + 1):\n",
    "                    node = beam_nodes.get()\n",
    "                    if node.ended:\n",
    "                        ended_list.append(np.array(node.tokens[0].numpy()))\n",
    "                        if len(ended_list) == sample:\n",
    "                            break\n",
    "                    else:\n",
    "                        tokens = node.tokens\n",
    "                        vocab_preds = self.model(tokens, colors)[:,-1:,:] # just distribution over last token\n",
    "                        log_probs, prediction_indices = vocab_preds.topk(beam_width, dim=2)  # taking the topk predictions\n",
    "                        for j in range(beam_width):\n",
    "                            prediction_index = prediction_indices[:,-1,j:j+1] # a single prediction\n",
    "                            log_prob = log_probs[0][0][j].item()\n",
    "                            updated_tokens = tokens.clone()\n",
    "                            updated_tokens = torch.cat((updated_tokens, prediction_index), dim=1)\n",
    "                            updated_log_prob = node.log_prob + log_prob\n",
    "                            ended = ((i == max_gen_len - 1) or (prediction_index.item() == caption[:, -1].item()))#.view(-1, 1)))\n",
    "                            new_node = BeamNode(updated_log_prob, updated_tokens, ended)\n",
    "                            beam_nodes.put(new_node)\n",
    "                if sample == 1: # for backwards compatability\n",
    "                    all_tokens.append(np.array(ended_list))\n",
    "                else:\n",
    "                    all_tokens.append(ended_list)\n",
    "        return all_tokens\n",
    "    \n",
    "    def train_iter(self, caption_tensor, color_tensor, target_tensor, criterion):\n",
    "        model_output = self.model(caption_tensor, color_tensor)\n",
    "        \n",
    "        #model_output = model_output[:, :-1, :].squeeze(0)\n",
    "        model_output = model_output.view(-1, self.model.vocab_size)\n",
    "        target_tensor = target_tensor.view(-1)\n",
    "        loss = criterion(model_output, target_tensor)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get DNC\n",
    "dnc_speaker = DNCSpeaker(DNCEncoder, num_epochs=5)\n",
    "dnc_speaker.init_model(vocab_size = 368, embed_dim=100, color_dim=3) # vocab size just copied from dnc training notebook\n",
    "# load model manually because we need to set \"map_location\" to cpu\n",
    "dnc_speaker.model.load_state_dict(torch.load(\"model_checkpoint_4.params\", map_location='cpu'))\n",
    "#dnc_speaker.load_model(\"./model_checkpoint_4.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "monroe_train_data = MonroeData(\"../color-evaluation/data/csv/train_corpus_monroe.csv\", \"../color-evaluation/data/entries/train_entries_monroe.pkl\")\n",
    "monroe_dev_data = MonroeData(\"../color-evaluation/data/csv/dev_corpus_monroe.csv\", \"../color-evaluation/data/entries/dev_entries_monroe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dnc color featurizer:\n",
    "def dnc_phi(color_list, space):\n",
    "    if space != \"hsv\":\n",
    "        print(\"Space must be hsv to use dnc\")\n",
    "        return None\n",
    "    # we're going to do a greedy search for the tokens and then max-pool their embeddings\n",
    "    features = [\n",
    "        [np.array([[0, 1]]), np.array([color_list])]\n",
    "    ]\n",
    "    predicted_tokens = dnc_speaker.predict(features) # looks like [array([[  0, 123,   1]])]\n",
    "    \n",
    "    predicted_tokens = predicted_tokens[0].flatten() # looks like array([  0, 123,   1])\n",
    "    \n",
    "    predicted_tokens = predicted_tokens[1:-1] # get rid of nasty start and end tokens\n",
    "    \n",
    "    # get embeddings for predicted tokens\n",
    "    print(predicted_tokens)\n",
    "    embeds = dnc.model.embed(torch.tensor.fromNumpy(predicted_tokens))\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = [\n",
    "        [np.array([[0, 1]]), np.array([[100.0, 30.0, 30.0]])]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[0, 1]]), array([[100.,  30.,  30.]])]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]])\n"
     ]
    }
   ],
   "source": [
    "test_predicted_tokens = dnc_speaker.predict([[np.array([[0, 1]]), np.array([[118.11320755,  66.52719665,  93.7254902 ]])]], beam_width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  0, 367,  32,  32,  32,  32,   4, 367,   4,  32,  32,  32, 118,\n",
       "          32,  20,  32, 367,  32, 212,  32, 212]])]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[0, 1]])[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([], dtype=float64)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[0, 1]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 2. Got 2 and 1 in dimension 0 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/wheel_3.6/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1333",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-264eb05e5895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnc_phi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-a9cde9b4202a>\u001b[0m in \u001b[0;36mdnc_phi\u001b[0;34m(color_list, space)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolor_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     ]\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpredicted_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnc_speaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# looks like [array([[  0, 123,   1]])]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpredicted_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# looks like array([  0, 123,   1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-9ba6efc1acf6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, sample, beam_width)\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                         \u001b[0mvocab_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# just distribution over last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# taking the topk predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benjaminnewman/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-44cd44b861f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, caption, color)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcolor_reps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# repeat color for number of tokens in captions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# concatenate colors to caption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cat along the innermost dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# dnc magic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mcontroller_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_vectors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 2. Got 2 and 1 in dimension 0 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/wheel_3.6/pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1333"
     ]
    }
   ],
   "source": [
    "embeds = dnc_phi([100.0, 30.0, 30.0], space=\"hsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing featurizers\n",
      "Obtaining training features\n"
     ]
    }
   ],
   "source": [
    "# OK because DNC isn't working, let's do this with the regular literal speaker/listener\n",
    "\n",
    "print(\"Initializing featurizers\")\n",
    "caption_phi = caption_featurizers.CaptionFeaturizer(tokenizer=caption_featurizers.EndingTokenizer) # Use with parameter files that end in `endings_tkn` - using endings tokenizer to separate endings like \"ish\" and \"er\"\n",
    "color_phi = ColorFeaturizer(color_phi_fourier, \"rgb\", normalized=True)\n",
    "\n",
    "# Now we have a different target function, because we want to predict the target color directly\n",
    "def target_color_target(data_entry):\n",
    "    return np.array(data_entry.colors[0].rgb_norm)\n",
    "\n",
    "feature_handler = FeatureHandler(monroe_train_data, monroe_dev_data, caption_phi, color_phi, target_fn=target_color_target,\n",
    "                                randomized_colors=False)\n",
    "\n",
    "print(\"Obtaining training features\") # get features even if you're runnning the pretrained model for example\n",
    "train_features = feature_handler.train_features()\n",
    "train_targets = feature_handler.train_targets()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_features = feature_handler.test_features()\n",
    "assess_targets = feature_handler.test_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0, 96,  6]),\n",
       " array([[ 1.0000000e+00,  2.0711137e-01, -9.1420978e-01, -1.2241068e-01,\n",
       "         -9.9631262e-01, -2.9028466e-01, -9.7003126e-01,  3.6807224e-02,\n",
       "          9.8527765e-01, -2.0711137e-01, -1.0000000e+00, -2.0711137e-01,\n",
       "         -9.4560730e-01,  1.2241068e-01,  9.9631262e-01,  4.3861625e-01,\n",
       "          9.7003126e-01, -3.6807224e-02, -9.1420978e-01,  2.0711137e-01,\n",
       "          1.0000000e+00,  5.1410276e-01,  9.4560730e-01, -1.2241068e-01,\n",
       "          7.8834641e-01, -4.3861625e-01, -9.7003126e-01,  0.0000000e+00,\n",
       "         -9.7831738e-01, -4.0524131e-01, -9.9247956e-01, -8.5797310e-02,\n",
       "          9.5694035e-01,  2.4298018e-01,  9.9932235e-01,  1.7096189e-01,\n",
       "         -9.7831738e-01, -1.2246469e-16,  9.7831738e-01,  3.2531029e-01,\n",
       "          9.9247956e-01,  8.5797310e-02,  8.9867449e-01, -2.4298018e-01,\n",
       "         -9.9932235e-01,  4.0524131e-01,  9.7831738e-01,  0.0000000e+00,\n",
       "          8.5772860e-01, -3.2531029e-01, -9.9247956e-01, -6.1523157e-01,\n",
       "         -8.9867449e-01,  2.4298018e-01],\n",
       "        [ 1.0000000e+00,  5.1410276e-01, -4.7139674e-01,  7.3564567e-02,\n",
       "         -8.1758481e-01, -9.1420978e-01, -9.8917651e-01, -6.3439327e-01,\n",
       "          3.3688986e-01, -5.1410276e-01, -1.0000000e+00, -5.1410276e-01,\n",
       "         -8.9322430e-01, -7.3564567e-02,  8.1758481e-01,  3.8268343e-01,\n",
       "          9.8917651e-01,  6.3439327e-01, -4.7139674e-01,  5.1410276e-01,\n",
       "          1.0000000e+00,  8.4485358e-01,  8.9322430e-01,  7.3564567e-02,\n",
       "          5.9569931e-01, -3.8268343e-01, -9.8917651e-01,  0.0000000e+00,\n",
       "         -8.5772860e-01, -8.8192129e-01, -9.9729043e-01, -5.7580817e-01,\n",
       "          4.0524131e-01, -1.4673047e-01,  7.7301043e-01,  9.4154406e-01,\n",
       "         -8.5772860e-01, -1.2246469e-16,  8.5772860e-01,  4.4961134e-01,\n",
       "          9.9729043e-01,  5.7580817e-01,  9.2387950e-01,  1.4673047e-01,\n",
       "         -7.7301043e-01,  8.8192129e-01,  8.5772860e-01,  0.0000000e+00,\n",
       "          5.3499764e-01, -4.4961134e-01, -9.9729043e-01, -8.0320752e-01,\n",
       "         -9.2387950e-01, -1.4673047e-01],\n",
       "        [ 1.0000000e+00, -7.8834641e-01,  2.4298018e-01, -8.7008697e-01,\n",
       "          3.8268343e-01,  2.6671275e-01,  5.1410276e-01,  1.2241068e-01,\n",
       "         -7.0710677e-01,  8.7008697e-01, -9.8917651e-01,  6.8954057e-01,\n",
       "         -1.0000000e+00,  7.8834641e-01, -2.4298018e-01,  8.7008697e-01,\n",
       "         -3.8268343e-01, -2.6671275e-01,  5.1410276e-01, -9.3299282e-01,\n",
       "          9.5694035e-01, -8.7008697e-01,  9.8917651e-01, -6.8954057e-01,\n",
       "          1.0000000e+00, -7.8834641e-01,  2.4298018e-01,  0.0000000e+00,\n",
       "         -6.1523157e-01,  9.7003126e-01, -4.9289820e-01,  9.2387950e-01,\n",
       "         -9.6377605e-01,  8.5772860e-01, -9.9247956e-01,  7.0710677e-01,\n",
       "         -4.9289820e-01, -1.4673047e-01,  7.2424710e-01, -1.2246469e-16,\n",
       "          6.1523157e-01, -9.7003126e-01,  4.9289820e-01, -9.2387950e-01,\n",
       "          9.6377605e-01, -8.5772860e-01,  3.5989505e-01,  2.9028466e-01,\n",
       "          4.9289820e-01,  1.4673047e-01, -7.2424710e-01,  0.0000000e+00,\n",
       "         -6.1523157e-01,  9.7003126e-01]], dtype=float32)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assess_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in assess_features:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
